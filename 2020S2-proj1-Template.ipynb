{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP90049 Introduction to Machine Learning, 2020 Semester 2\n",
    "-----\n",
    "## Project 1: Predicting stroke with Naive Bayes and K-NN\n",
    "-----\n",
    "###### Student Name(s): Wildan Anugrah\n",
    "###### Python version: Python 3.6.9\n",
    "###### Submission deadline: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Project 1 submission. \n",
    "\n",
    "Marking will be applied on the functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Newer versions\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should transform data into a usable format \n",
    "def convert_class_to_integer(x):\n",
    "        return int(x)\n",
    "\n",
    "def convert_feature_gender(x):\n",
    "        if x == \"Female\": return 0\n",
    "        elif x == \"Male\": return 1\n",
    "        else: return 999\n",
    "\n",
    "def convert_feature_ever_married(x):\n",
    "        if x == \"No\": return 0\n",
    "        elif x == \"Yes\": return 1\n",
    "        else: return 999\n",
    "\n",
    "def convert_feature_work_type(x):\n",
    "        if x == \"Govt_job\": return 0\n",
    "        elif x == \"Private\": return 1\n",
    "        elif x == \"Self-employed\": return 2\n",
    "        elif x == \"children\": return 3\n",
    "        elif x == \"Never_worked\": return 4\n",
    "        else: return 999\n",
    "\n",
    "def convert_feature_Residence_type(x):\n",
    "        if x == \"Rural\": return 0\n",
    "        elif x == \"Urban\": return 1\n",
    "        else: return 999\n",
    "\n",
    "def convert_feature_smoking_status(x):\n",
    "        if x == \"formerly smoked\": return 0\n",
    "        elif x == \"never smoked\": return 1\n",
    "        elif x == \"smokes\": return 2\n",
    "        else: return 999\n",
    "\n",
    "def preprocess(filename):\n",
    "    attributes = []\n",
    "    labels = []\n",
    "    with open(filename, mode='r') as fin:\n",
    "        for line in fin:\n",
    "            atts = line.strip().split(\",\")\n",
    "            attributes.append(atts[:-1]) #all atts, excluding the class\n",
    "            labels.append(atts[-1])\n",
    "    \n",
    "    # remove header\n",
    "    attributes = attributes[1:]\n",
    "    labels = labels[1:]\n",
    "    \n",
    "    attributes_ordinal = []\n",
    "    for x in attributes:\n",
    "        f1, f2, f3, f4, f5, f6, f7, f8, f9, f10 = x\n",
    "        f1 = float(f1) # avg_glucose_level\n",
    "        f2 = float(f2) #bmi\n",
    "        f3 = int(f3) #age\n",
    "        f4 = convert_feature_gender(f4) #gender\n",
    "        f5 = int(f5) #hypertension\n",
    "        f6 = int(f6) #heart_disease\n",
    "        f7 = convert_feature_ever_married(f7) #ever_married\n",
    "        f8 = convert_feature_work_type(f8) #work_type\n",
    "        f9 = convert_feature_Residence_type(f9) #Residence_type\n",
    "        f10 = convert_feature_smoking_status(f10) #smoking_status\n",
    "        x = [f1, f2, f3, f4, f5, f6, f7, f8, f9, f10]\n",
    "        attributes_ordinal.append(x)\n",
    "        \n",
    "    attributes = attributes_ordinal\n",
    "    \n",
    "    labels_ordinal = []\n",
    "    for x in labels:\n",
    "        x = convert_class_to_integer(x)\n",
    "        labels_ordinal.append(x)\n",
    "        \n",
    "    labels = labels_ordinal\n",
    "    \n",
    "    #make sure everything is Integer\n",
    "    attributes = np.array(attributes, dtype='int')\n",
    "    labels = np.array(labels, dtype='int')\n",
    "    \n",
    "    return attributes.tolist(), labels.tolist()\n",
    "\n",
    "#preprocess(\"stroke_update.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should split a data set into a training set and hold-out test set\n",
    "def split_data(attributes, labels):\n",
    "        return train_test_split(attributes, labels, test_size=0.2)\n",
    "\n",
    "#attributes, labels = preprocess(\"stroke_update.csv\")\n",
    "#split_data(attributes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "# Function for counting the frequency of classes to claculate prior probability p(y=i) = n(i)/N\n",
    "def p_y(y):\n",
    "    class_priors = [0]*len(set(y))\n",
    "    for c in y:\n",
    "        class_priors[c]+=1    \n",
    "    return class_priors\n",
    "\n",
    "# Function for likelihood p(x=j|y=i) = n(i,j)/n(i)\n",
    "def p_xy(x,y):\n",
    "    # init dict (over classes) of dict (over features) of dict (over value counts)\n",
    "    outdict = {c:{} for c in y}\n",
    "    for d in outdict.keys():\n",
    "        for f in range(len(x[0])):\n",
    "            outdict[d][f]={}\n",
    "            rng = set([i[f] for i in x])\n",
    "            outdict[d][f] = {v:0 for v in rng}\n",
    "    \n",
    "      \n",
    "    # fill dict with counts\n",
    "    for idx,_ in enumerate(x):\n",
    "        for fidx, _ in enumerate(x[idx]):\n",
    "            outdict[y[idx]][fidx][x[idx][fidx]]+=1\n",
    "    \n",
    "    # normalize, or fill in epsilons as needed\n",
    "    for cl in outdict.keys():\n",
    "        for f in outdict[cl].keys():\n",
    "            for val in outdict[cl][f]:\n",
    "                if outdict[cl][f][val] > 0:\n",
    "                    outdict[cl][f][val] = outdict[cl][f][val] / p_y(y)[cl]\n",
    "\n",
    "            \n",
    "    return outdict\n",
    "\n",
    "def train(x, pc, pxc):\n",
    "    class_probs = []\n",
    "    for y in range(len(pc)):\n",
    "        class_prob=pc[y]/sum(pc)\n",
    "        for fidx, f in enumerate(x):\n",
    "            if f in pxc[y][fidx]:\n",
    "                class_prob = class_prob * pxc[y][fidx][f]\n",
    "        class_probs.append(class_prob)\n",
    "    return class_probs, np.argmax([class_probs])\n",
    "\n",
    "#for x in X_train:\n",
    "#    print(train(x, py, pxy))\n",
    "#pxy = p_xy(X_train, y_train)\n",
    "#pp = pprint.PrettyPrinter(indent=4)\n",
    "#pp.pprint(pxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(x, pc, pxc):\n",
    "    class_probs = []\n",
    "    for y in range(len(pc)):\n",
    "        class_prob=pc[y]/sum(pc)\n",
    "        for fidx, f in enumerate(x):\n",
    "            if f in pxc[y][fidx]:\n",
    "                class_prob = class_prob * pxc[y][fidx][f]\n",
    "        class_probs.append(class_prob)\n",
    "    return class_probs, np.argmax([class_probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions in terms of metrics\n",
    "# Function to evaluate a set of predictions in terms of metrics\n",
    "from sklearn import metrics\n",
    "def evaluate(pred,true):\n",
    "    CM = metrics.confusion_matrix(true, pred) # Confusion Matrix\n",
    "    Acc = metrics.accuracy_score(true, pred) # Accuracy\n",
    "    precf1 = metrics.precision_recall_fscore_support(true, pred) # Precision, Recall and F1-score\n",
    "    return CM, Acc, precf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluation using training data\n",
      "Confusion Matrix:\n",
      "[[1559  197]\n",
      " [ 180  256]]\n",
      "accuracy: 0.8280109489051095\n",
      "accuracy by sklearn.metric: 0.8280109489051095\n",
      "precision: [0.89649224 0.56512141]\n",
      "recall: [0.88781321 0.58715596]\n",
      "F1: [0.89213162 0.57592801]\n",
      "\n",
      "evaluation using test data\n",
      "Confusion Matrix:\n",
      "[[390  46]\n",
      " [ 75  37]]\n",
      "accuracy: 0.7791970802919708\n",
      "accuracy by sklearn.metric: 0.7791970802919708\n",
      "precision: [0.83870968 0.44578313]\n",
      "recall: [0.89449541 0.33035714]\n",
      "F1: [0.86570477 0.37948718]\n"
     ]
    }
   ],
   "source": [
    "attr, labels = preprocess(\"stroke_update.csv\")\n",
    "X_train, X_test, y_train, y_test = split_data(attr, labels)\n",
    "pxy = p_xy(X_train, y_train)\n",
    "py = p_y(y_train)\n",
    "\n",
    "print(\"\\nevaluation using training data\")\n",
    "\n",
    "correct = 0\n",
    "preds = []\n",
    "for i in range(len(X_train)):\n",
    "    prediction = predict(X_train[i], py, pxy)[1]\n",
    "    correct = correct + int(prediction==y_train[i])\n",
    "    preds.append(prediction)                 \n",
    "CM, Acc, precf1 = evaluate(preds, y_train)\n",
    "\n",
    "print(\"Confusion Matrix:\\n{}\\naccuracy: {}\\naccuracy by sklearn.metric: {}\\nprecision: {}\\nrecall: {}\\nF1: {}\".format(CM,\n",
    "                                                correct / len(X_train), \n",
    "                                                Acc,\n",
    "                                                precf1[0],\n",
    "                                                precf1[1],\n",
    "                                                precf1[2]))\n",
    "\n",
    "# predict on test\n",
    "print(\"\\nevaluation using test data\")\n",
    "\n",
    "correct = 0\n",
    "preds = []\n",
    "for i in range(len(X_test)):\n",
    "    prediction = predict(X_test[i], py, pxy)[1]\n",
    "    correct = correct + int(prediction==y_test[i])\n",
    "    preds.append(prediction)            \n",
    "CM, Acc, precf1 = evaluate(preds, y_test)\n",
    "\n",
    "print(\"Confusion Matrix:\\n{}\\naccuracy: {}\\naccuracy by sklearn.metric: {}\\nprecision: {}\\nrecall: {}\\nF1: {}\".format(CM, \n",
    "                                                correct / len(X_test), \n",
    "                                                Acc,\n",
    "                                                precf1[0],\n",
    "                                                precf1[1],\n",
    "                                                precf1[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions (you may respond in a cell or cells below):\n",
    "\n",
    "You should respond to questions 1-4. In question 2 (b) you can choose between two options. A response to a question should take about 100--200 words, and make reference to the data wherever possible.\n",
    "\n",
    "### Question 1: Data exploration\n",
    "\n",
    "- a) Explore the data and summarise different aspects of the data. Can you see any interesting characteristic in features, classes or categories? What is the main issue with the data? Considering the issue, how would the Naive Bayes classifier work on this data? Discuss your answer based on the Naive Bayes' formulation.\n",
    "- b) Is accuracy an appropriate metric to evaluate the models created for this data? Justify your answer. Explain which metric(s) would be more appropriate, and contrast their utility against accuracy. [no programming required]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Naive Bayes concepts and formulation\n",
    "\n",
    "- a) Explain the independence assumption underlying Naive Bayes. What are the advantages and disadvantages of this assumption? Elaborate your answers using the features of the provided data. [no programming required]\n",
    "- b) Implement the Naive Bayes classifier. You need to decide how you are going to apply Naive Bayes for nominal and numeric attributes. You can combine both Gaussian and Categorical Naive Bayes (option 1) or just using Categorical Naive Bayes (option 2). Explain your decision. For Categorical Naive Bayes, you can choose either epsilon or Laplace smoothing for this calculation. Evaluate the classifier using accuracy and appropriate metric(s) on test data. Explain your observations on how the classifiers have performed based on the metric(s). Discuss the performance of the classifiers in comparison with the Zero-R baseline.\n",
    "- c) Explain the difference between epsilon and Laplace smoothing. [no programming required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Model Comparison\n",
    "- a) Implement the K-NN classifier, and find the optimal value for K. \n",
    "- b) Based on the obtained value for K in question 4 (a), evaluate the classifier using accuracy and chosen metric(s) on test data. Explain your observations on how the classifiers have performed based on the metric(s). Discuss the performance of the classifiers in comparison with the Zero-R baseline.\n",
    "- c) Is K-NN sensitive to imbalanced data? Justify your answer. [no programming required]\n",
    "- d) Compare the classifiers (Naive Bayes and K-NN) based on metrics' results. Provide a comparatory discussion on the results. [no programming required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
